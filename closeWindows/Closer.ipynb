{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c89eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np\n",
    "# import cv2 as cv\n",
    "# import pyautogui\n",
    "# import uuid\n",
    "# import os\n",
    "# import time\n",
    "# IMAGES_PATH = os.path.join('Data', 'Images')\n",
    "# number_imgs = 500\n",
    "# screenWidth, screenHeight = 3456, 2234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee92aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 0\n",
      "Part 1\n",
      "Collecting image number 1\n",
      "Collecting image number 2\n",
      "Collecting image number 3\n",
      "Collecting image number 4\n",
      "Collecting image number 5\n",
      "Collecting image number 6\n",
      "Collecting image number 7\n",
      "Collecting image number 8\n",
      "Collecting image number 9\n",
      "Collecting image number 10\n",
      "Collecting image number 11\n",
      "Collecting image number 12\n",
      "Collecting image number 13\n",
      "Collecting image number 14\n",
      "Collecting image number 15\n",
      "Collecting image number 16\n",
      "Collecting image number 17\n",
      "Collecting image number 18\n",
      "Collecting image number 19\n",
      "Collecting image number 20\n",
      "Collecting image number 21\n",
      "Collecting image number 22\n",
      "Collecting image number 23\n",
      "Collecting image number 24\n",
      "Collecting image number 25\n",
      "Collecting image number 26\n",
      "Collecting image number 27\n",
      "Collecting image number 28\n",
      "Collecting image number 29\n",
      "Collecting image number 30\n",
      "Collecting image number 32\n",
      "Collecting image number 33\n",
      "Collecting image number 34\n",
      "Collecting image number 35\n",
      "Collecting image number 36\n",
      "Collecting image number 37\n",
      "Collecting image number 38\n",
      "Collecting image number 39\n",
      "Collecting image number 40\n",
      "Collecting image number 41\n",
      "Collecting image number 42\n",
      "Collecting image number 43\n",
      "Collecting image number 44\n",
      "Collecting image number 45\n",
      "Collecting image number 46\n",
      "Collecting image number 47\n",
      "Collecting image number 48\n",
      "Collecting image number 49\n",
      "Collecting image number 50\n",
      "Collecting image number 51\n",
      "Collecting image number 52\n",
      "Collecting image number 53\n",
      "Collecting image number 54\n",
      "Collecting image number 55\n",
      "Collecting image number 56\n",
      "Collecting image number 57\n",
      "Collecting image number 58\n",
      "Collecting image number 59\n",
      "Collecting image number 60\n",
      "Collecting image number 61\n",
      "Collecting image number 62\n",
      "Collecting image number 63\n",
      "Collecting image number 64\n",
      "Collecting image number 65\n",
      "Collecting image number 66\n",
      "Collecting image number 67\n",
      "Collecting image number 68\n",
      "Collecting image number 69\n",
      "Collecting image number 70\n",
      "Collecting image number 71\n",
      "Collecting image number 72\n",
      "Collecting image number 73\n",
      "Collecting image number 74\n",
      "Collecting image number 75\n",
      "Collecting image number 76\n",
      "Collecting image number 77\n",
      "Collecting image number 78\n",
      "Collecting image number 79\n",
      "Collecting image number 80\n",
      "Collecting image number 82\n",
      "Collecting image number 83\n",
      "Collecting image number 84\n",
      "Collecting image number 85\n",
      "Collecting image number 86\n",
      "Collecting image number 87\n",
      "Collecting image number 88\n",
      "Collecting image number 89\n",
      "Collecting image number 90\n",
      "Collecting image number 91\n",
      "Collecting image number 92\n",
      "Collecting image number 93\n",
      "Collecting image number 94\n",
      "Collecting image number 95\n",
      "Collecting image number 96\n",
      "Collecting image number 97\n",
      "Collecting image number 98\n",
      "Collecting image number 99\n",
      "Collecting image number 100\n",
      "Collecting image number 101\n",
      "Collecting image number 102\n",
      "Collecting image number 103\n",
      "Collecting image number 104\n",
      "Collecting image number 105\n",
      "Collecting image number 106\n",
      "Collecting image number 107\n",
      "Collecting image number 108\n",
      "Collecting image number 109\n",
      "Collecting image number 110\n",
      "Collecting image number 111\n",
      "Collecting image number 112\n",
      "Collecting image number 113\n",
      "Collecting image number 114\n",
      "Collecting image number 115\n",
      "Collecting image number 116\n",
      "Collecting image number 117\n",
      "Collecting image number 118\n",
      "Collecting image number 119\n",
      "Collecting image number 120\n",
      "Collecting image number 121\n",
      "Collecting image number 122\n",
      "Collecting image number 123\n",
      "Collecting image number 124\n",
      "Collecting image number 125\n",
      "Collecting image number 126\n",
      "Collecting image number 127\n",
      "Collecting image number 128\n",
      "Collecting image number 129\n",
      "Collecting image number 130\n",
      "Collecting image number 132\n",
      "Collecting image number 133\n",
      "Collecting image number 134\n",
      "Collecting image number 135\n",
      "Collecting image number 136\n",
      "Collecting image number 137\n",
      "Collecting image number 138\n",
      "Collecting image number 139\n",
      "Collecting image number 140\n",
      "Collecting image number 141\n",
      "Collecting image number 142\n",
      "Collecting image number 143\n",
      "Collecting image number 144\n",
      "Collecting image number 145\n",
      "Collecting image number 146\n",
      "Collecting image number 147\n",
      "Collecting image number 148\n",
      "Collecting image number 149\n",
      "Collecting image number 150\n",
      "Collecting image number 151\n",
      "Collecting image number 152\n",
      "Collecting image number 153\n",
      "Collecting image number 154\n",
      "Collecting image number 155\n",
      "Collecting image number 156\n",
      "Collecting image number 157\n",
      "Collecting image number 158\n",
      "Collecting image number 159\n",
      "Collecting image number 160\n",
      "Collecting image number 161\n",
      "Collecting image number 162\n",
      "Collecting image number 163\n",
      "Collecting image number 164\n",
      "Collecting image number 165\n",
      "Collecting image number 166\n",
      "Collecting image number 167\n",
      "Collecting image number 168\n",
      "Collecting image number 169\n",
      "Collecting image number 170\n",
      "Collecting image number 171\n",
      "Collecting image number 172\n",
      "Collecting image number 173\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPart \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(img_num\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     14\u001b[0m cv\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for img_num in range(20+number_imgs):\n",
    "    screen = pyautogui.screenshot()\n",
    "    screen_array = np.array(screen)\n",
    "    cropped_region = screen_array[:, screenWidth//2:, :]\n",
    "    corrected_colors = cv.cvtColor(cropped_region, cv.COLOR_RGB2BGR)\n",
    "    cv.imshow('YOLO', corrected_colors)\n",
    "    img_name = os.path.join(IMAGES_PATH, str(uuid.uuid1())+'.jpg')\n",
    "    if img_num > 19:\n",
    "        if img_num%50:\n",
    "            print('Collecting image number {}'.format(img_num-19))\n",
    "        cv.imwrite(img_name, corrected_colors)\n",
    "    elif not img_num%10:\n",
    "        print('Part {}'.format(img_num//10))\n",
    "    cv.waitKey(1)\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8274ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
      "/Users/adilbhatti/pygui/closeWindows/yolov5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5943b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=datasetClose.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=500, batch_size=16, imgsz=500, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=16, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 3 (delta 0), reused 1 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (3/3), 9.32 KiB | 795.00 KiB/s, done.\n",
      "From https://github.com/ultralytics/yolov5\n",
      "   4e59377..a770b2e  v8_banner  -> origin/v8_banner\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-72-g064365d Python-3.9.12 torch-1.13.1 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=16\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
      "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1     84861  models.yolo.Detect                      [16, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "Model summary: 291 layers, 20931933 parameters, 20931933 gradients, 48.4 GFLOPs\n",
      "\n",
      "Transferred 475/481 items from yolov5m.pt\n",
      "WARNING ‚ö†Ô∏è --img-size 500 must be multiple of max stride 32, updating to 512\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adilbhatti/pygui/closeWindows/Data/Images... 0 images, 50\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è No labels found in /Users/adilbhatti/pygui/closeWindows/Data/Images.cache. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/adilbhatti/pygui/closeWindows/Data/Images.cache\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/adilbhatti/pygui/closeWindows/yolov5/train.py\", line 634, in <module>\n",
      "    main(opt)\n",
      "  File \"/Users/adilbhatti/pygui/closeWindows/yolov5/train.py\", line 528, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/Users/adilbhatti/pygui/closeWindows/yolov5/train.py\", line 187, in train\n",
      "    train_loader, dataset = create_dataloader(train_path,\n",
      "  File \"/Users/adilbhatti/pygui/closeWindows/yolov5/utils/dataloaders.py\", line 124, in create_dataloader\n",
      "    dataset = LoadImagesAndLabels(\n",
      "  File \"/Users/adilbhatti/pygui/closeWindows/yolov5/utils/dataloaders.py\", line 502, in __init__\n",
      "    assert nf > 0 or not augment, f'{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}'\n",
      "AssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /Users/adilbhatti/pygui/closeWindows/Data/Images.cache, can not start training. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5 && python3 train.py --img 500 --batch 16 --epochs 500 --data datasetClose.yaml --weights yolov5m.pt --workers 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0b0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov5' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ebcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /Users/adilbhatti/.cache/torch/hub/master.zip\n",
      "YOLOv5 üöÄ 2023-2-2 Python-3.9.12 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20913549 parameters, 0 gradients, 48.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp13/weights/last.pt', force_reload = True)\n",
    "while True:\n",
    "    screen = pyautogui.screenshot()\n",
    "    screen_array = np.array(screen)\n",
    "    # height, width, channels\n",
    "    cropped_region = screen_array[:, screenWidth//2:, :]\n",
    "    #convert to BGR\n",
    "    corrected_colors = cv.cvtColor(cropped_region, cv.COLOR_RGB2BGR)\n",
    "    \n",
    "    results = model(corrected_colors)\n",
    "    cv.imshow('YOLO', np.squeeze(results.render()))\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "026e386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /Users/adilbhatti/.cache/torch/hub/master.zip\n",
      "YOLOv5 üöÄ 2023-2-2 Python-3.9.12 torch-1.13.1 CPU\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[Errno 2] No such file or directory: 'yolov5/runs/train/exp6/weights/last.pt'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:49\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:345\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax()), \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:79\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[0;32m---> 79\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5/runs/train/exp6/weights/last.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:60\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:79\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[0;32m---> 79\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov5/runs/train/exp6/weights/last.pt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5/runs/train/exp6/weights/last.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/hub.py:542\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    539\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    540\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 542\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/hub.py:572\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    571\u001b[0m entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 572\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremove(hubconf_dir)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:83\u001b[0m, in \u001b[0;36mcustom\u001b[0;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# YOLOv5 custom or local model\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:78\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/ultralytics/yolov5/issues/36\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     77\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: [Errno 2] No such file or directory: 'yolov5/runs/train/exp6/weights/last.pt'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help."
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp6/weights/last.pt', force_reload = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce18ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp7/weights/best.pt, cfg=, data=datasetClose.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=32, imgsz=672, rect=False, resume=True, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=6, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "remote: Enumerating objects: 45, done.\u001b[K\n",
      "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 45 (delta 21), reused 42 (delta 21), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (45/45), 66.05 KiB | 1024.00 KiB/s, done.\n",
      "From https://github.com/ultralytics/yolov5\n",
      "   064365d..d02ee60  master     -> origin/master\n",
      "   391b4df..48df0b0  docker     -> origin/docker\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 2 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 üöÄ v7.0-72-g064365d Python-3.9.12 torch-1.13.1 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
      "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1     84861  models.yolo.Detect                      [16, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "Model summary: 291 layers, 20931933 parameters, 20931933 gradients, 48.4 GFLOPs\n",
      "\n",
      "Transferred 481/481 items from runs/train/exp21/weights/last.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n",
      "Resuming training from runs/train/exp21/weights/last.pt from epoch 38 to 200 total epochs\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adilbhatti/pygui/closeWindows/Data/Images.cache... 618 im\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adilbhatti/pygui/closeWindows/Data/Images.cache... 618 imag\u001b[0m\n",
      "Plotting labels to runs/train/exp21/labels.jpg... \n",
      "Image sizes 672 train, 672 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp21\u001b[0m\n",
      "Starting training for 200 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "     38/199         0G    0.02918   0.009561    0.00176         86        672:  ^C\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5 && python3 train.py --img 672 --batch 32 --epochs 100 --data datasetClose.yaml --weights runs/train/exp7/weights/best.pt --workers 6 --resume\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a4ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /Users/adilbhatti/.cache/torch/hub/master.zip\n",
      "YOLOv5 üöÄ 2023-2-5 Python-3.9.12 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20913549 parameters, 0 gradients, 48.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 248.8627471923828 102.6764907836914\n",
      "\n",
      "\n",
      " 250.94552612304688 104.5656967163086\n",
      "\n",
      "\n",
      " 250.77682495117188 104.22794342041016\n",
      "\n",
      "\n",
      " 250.7669677734375 104.22459411621094\n",
      "\n",
      "\n",
      " 250.7669677734375 104.22459411621094\n",
      "\n",
      "\n",
      " 249.41331481933594 103.2713851928711\n",
      "\n",
      "\n",
      " 248.5552520751953 102.57926940917969\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m results \u001b[38;5;241m=\u001b[39m model(corrected_colors)\n\u001b[1;32m     16\u001b[0m cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLO\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msqueeze(results\u001b[38;5;241m.\u001b[39mrender()))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxyxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), results\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pyautogui\n",
    "import torch\n",
    "screenWidth, screenHeight = 3456, 2234\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp21/weights/last.pt', force_reload = True)\n",
    "while True:\n",
    "    screen = pyautogui.screenshot()\n",
    "    screen_array = np.array(screen)\n",
    "    # height, width, channels\n",
    "    cropped_region = screen_array[:, screenWidth//2:, :]\n",
    "    #convert to BGR\n",
    "    corrected_colors = cv.cvtColor(cropped_region, cv.COLOR_RGB2BGR)\n",
    "    \n",
    "    results = model(corrected_colors)\n",
    "    cv.imshow('YOLO', np.squeeze(results.render()))\n",
    "    print('\\n', results.xyxy[0][0][0].item(), results.xyxy[0][0][1].item())\n",
    "    print()\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dfce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
